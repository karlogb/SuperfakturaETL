# SuperFaktura ETL Pipeline

This repository contains an ETL (Extract, Transform, Load) pipeline for processing data from SuperFaktura and uploading it to Google BigQuery. It is designed for periodic data synchronization and analysis.

## ðŸ”§ Features

- Extraction of invoice, client, and expense data from SuperFaktura API.
- Transformation and preprocessing of data in the landing zone.
- Incremental and full data loading support.
- Uploading processed data to BigQuery.
- Configurable and modular structure for easy maintenance and scalability.

## ðŸ—‚ Project Structure

```
etl/
â”œâ”€â”€ keys/
â”‚   â””â”€â”€ serviceAccount/
â”‚       â””â”€â”€ serviceAccount.json       # Google Cloud service account credentials
â”œâ”€â”€ superfaktura/
â”‚   â”œâ”€â”€ app.py                        # Main app initialization
â”‚   â”œâ”€â”€ config.py                     # Configuration settings (API keys, project IDs, etc.)
â”‚   â”œâ”€â”€ key.json                      # SuperFaktura API credentials
â”‚   â”œâ”€â”€ main.py                       # Entry point of the ETL process
â”‚   â”œâ”€â”€ utils.py                      # Helper functions
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ bigquery_uploader.py      # Uploading transformed data to BigQuery
â”‚       â”œâ”€â”€ landing_preprocess_all.py# Full load preprocessing
â”‚       â”œâ”€â”€ landing_preprocess_inc.py# Incremental load preprocessing
â”‚       â”œâ”€â”€ present.py                # Presentation layer logic
```

## ðŸš€ Getting Started

### Prerequisites

- Python 3.9+
- Google Cloud project with BigQuery enabled
- SuperFaktura account with API access

### Installation

1. Clone this repository:

```bash
git clone https://github.com/yourusername/superfaktura-etl.git
cd superfaktura-etl
```

2. Create a virtual environment and activate it:

```bash
python -m venv venv
source venv/bin/activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Set up configuration:

- Add your SuperFaktura API credentials to `key.json`.
- Add your GCP service account JSON to `keys/serviceAccount/serviceAccount.json`.

### Running the Pipeline

Simply run:

```bash
python superfaktura/main.py
```

This will trigger the full ETL process including data extraction, transformation, and upload to BigQuery.

## ðŸ§  Notes

- Make sure your GCP service account has permission to access and write to BigQuery datasets.
- This pipeline can be scheduled via cron jobs, Airflow, or any orchestration tool of choice.
